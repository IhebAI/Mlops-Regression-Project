{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:54:32.629939600Z",
     "start_time": "2024-07-09T14:54:32.626990500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:54:32.641301100Z",
     "start_time": "2024-07-09T14:54:32.630945200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\iheba\\\\IdeaProjects\\\\Mlops-Regression-Project\\\\research'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:54:32.647813100Z",
     "start_time": "2024-07-09T14:54:32.640300600Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:54:32.672356400Z",
     "start_time": "2024-07-09T14:54:32.644814100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\iheba\\\\IdeaProjects\\\\Mlops-Regression-Project'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:54:32.672356400Z",
     "start_time": "2024-07-09T14:54:32.652829100Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_URL: str\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path\n",
    "    expected_hash: str\n",
    "    status_file:Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:54:33.104242300Z",
     "start_time": "2024-07-09T14:54:32.658357200Z"
    }
   },
   "outputs": [],
   "source": [
    "from RegressionProject.constants import *\n",
    "from RegressionProject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:54:33.111071100Z",
     "start_time": "2024-07-09T14:54:33.108242800Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath=CONFIG_FILE_PATH,\n",
    "            params_filepath=PARAMS_FILE_PATH,\n",
    "            schema_filepath=SCHEMA_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir,\n",
    "            expected_hash=config.expected_hash,\n",
    "            status_file=config.status_file\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:54:33.120002100Z",
     "start_time": "2024-07-09T14:54:33.112075600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "import zipfile\n",
    "from RegressionProject.logging import logger\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:02:11.232365700Z",
     "start_time": "2024-07-09T15:02:11.219933100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def calculate_file_hash(self, file_path):\n",
    "        \"\"\"Calculate the SHA-256 hash of a file.\"\"\"\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "\n",
    "    def download_file(self):\n",
    "        print(self.config.source_URL)\n",
    "\n",
    "        if os.path.exists(self.config.local_data_file):\n",
    "            logger.info(f\"File already exists of size: {os.path.getsize(self.config.local_data_file)}\")\n",
    "        else:\n",
    "            # Download the file\n",
    "            filename, headers = request.urlretrieve(\n",
    "                url=self.config.source_URL,\n",
    "                filename=self.config.local_data_file\n",
    "            )\n",
    "            logger.info(f\"{filename} downloaded with the following info:\\n{headers}\")\n",
    "        # Validate file hash after download\n",
    "        if self.validate_file_hash():\n",
    "            print(\"File integrity verified.\")\n",
    "            download_date = datetime.now().isoformat()\n",
    "            self.write_verification_status(True, download_date)\n",
    "        else:\n",
    "            print(\"File integrity verification failed.\")\n",
    "            self.write_verification_status(False, None)\n",
    "            # Handle error or raise an exception\n",
    "\n",
    "    def write_verification_status(self, status, download_date):\n",
    "        \"\"\"Write verification status and download date to a text file.\"\"\"\n",
    "        with open(self.config.status_file, 'w') as f:\n",
    "            f.write(f\"Verification Status: {'True' if status else 'False'}\\n\")\n",
    "            if download_date:\n",
    "                f.write(f\"Download Date: {download_date}\\n\")\n",
    "        print(f\"Verification status written to {self.config.status_file}\")\n",
    "        \n",
    "    def validate_file_hash(self):\n",
    "        \"\"\"Validate the hash of the downloaded file.\"\"\"\n",
    "        expected_hash = self.config.expected_hash  # Replace with your actual expected hash\n",
    "        downloaded_hash = self.calculate_file_hash(self.config.local_data_file)\n",
    "        return downloaded_hash == expected_hash\n",
    "    \n",
    "    def extract_zip_file(self):\n",
    "        \"\"\"\n",
    "        zip_file_path: str\n",
    "        Extracts the zip file into the data directory\n",
    "        Function returns None\n",
    "        \"\"\"\n",
    "        unzip_path = self.config.unzip_dir\n",
    "        os.makedirs(unzip_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(self.config.local_data_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:02:11.894441600Z",
     "start_time": "2024-07-09T15:02:11.878120300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-09 08:02:11,879: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-07-09 08:02:11,883: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-07-09 08:02:11,884: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-07-09 08:02:11,885: INFO: common: created directory at: artifacts]\n",
      "[2024-07-09 08:02:11,886: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "https://github.com/IhebAI/Datasets/raw/main/student.zip\n",
      "[2024-07-09 08:02:11,887: INFO: 3206609817: File already exists of size: 7385]\n",
      "File integrity verified.\n",
      "Verification status written to artifacts/data_ingestion/status.txt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.download_file()\n",
    "    data_ingestion.extract_zip_file()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T14:54:33.152054500Z",
     "start_time": "2024-07-09T14:54:33.148372300Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
